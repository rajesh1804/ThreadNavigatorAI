import streamlit as st
from scripts.agent_graph import run_graph
from scripts.eval_samples import sample_options
import time

# ------------------ Streamlit Config ------------------
st.set_page_config(page_title="ThreadNavigatorAI", layout="centered")

# ------------------ Branding Header ------------------
st.markdown("""
# ğŸ§µ ThreadNavigatorAI
**Navigate Reddit-style threads using LLM agents.**
""")
# st.image("https://raw.githubusercontent.com/rajesh1804/ThreadNavigatorAI/main/assets/banner_threadnavigatorai.png", use_column_width=True)

# ------------------ Sidebar: What this app does ------------------
with st.sidebar:
    st.markdown("## ğŸ§  What is ThreadNavigatorAI?")
    st.markdown("""
- ğŸ¤– AI-powered Reddit-style thread summarizer  
- ğŸ›¡ï¸ Moderates for sarcasm, trolling, bias, off-topic  
- ğŸ’¬ Suggests personalized replies  
- ğŸ” Built using LangGraph (Agentic RAG), OpenRouter, and Weaviate  
- ğŸš€ Deployed on Hugging Face Spaces (Free Tier)
    """)
    st.markdown("---")
    st.markdown("Made by [Rajesh](https://github.com/rajesh1804) | Powered by OpenRouter + LangGraph")

# ------------------ Problem Framing ------------------
st.markdown("""
### ğŸ’¡ Why ThreadNavigatorAI?

Reddit-style threads are chaotic.  
Trolling, sarcasm, off-topic comments â€” it's easy to lose the signal in the noise.

**ThreadNavigatorAI** uses **agentic LLM pipelines** to:
- Summarize long threads smartly
- Detect moderation-worthy content
- Suggest intelligent follow-up replies

""")

# ------------------ ğŸ”¥ Instant Demo ------------------
if st.button("ğŸ”¥ Try Smart Demo"):
    thread_id = "thread_002"
    query = "Summarize this thread and point out any sarcasm or trolling"
    with st.spinner("Running LangGraph agents..."):
        try:
            start_time = time.time()
            result = run_graph(thread_id=thread_id, user_query=query)
            latency = round(time.time() - start_time, 2)

            st.success("âœ… Demo Complete!")

            st.subheader("ğŸ“ Smart Summary")
            st.markdown(result["summary"])

            st.subheader("ğŸ›¡ï¸ Moderation Report")
            st.markdown(result["moderation_report"])

            st.subheader("ğŸ’¬ Suggested Reply")
            st.markdown(result["reply_suggestion"])

            st.subheader("â±ï¸ Latency")
            st.success(f"{latency} seconds")

        except Exception as e:
            st.error(f"âŒ Error: {str(e)}")

# ------------------ ğŸ§µ Full Agent Workflow ------------------
st.markdown("---")
st.subheader("ğŸ§  Run Agentic Workflow on Any Thread")

thread_id = st.selectbox("ğŸ“‚ Choose a thread", ["thread_001", "thread_002", "thread_003", "thread_004"])

query = st.text_input("ğŸ’¬ Ask your question about the thread",
                      placeholder="e.g. What's the main takeaway? Any bias?")

submit = st.button("ğŸš€ Analyze Thread")

if submit and thread_id and query:
    with st.spinner("Running LangGraph agent pipeline..."):
        try:
            start_time = time.time()
            result = run_graph(thread_id=thread_id, user_query=query)
            latency = round(time.time() - start_time, 2)

            st.success("âœ… Analysis Complete!")

            st.subheader("ğŸ“ Smart Summary")
            st.markdown(result["summary"])

            st.subheader("ğŸ›¡ï¸ Moderation Report")
            st.markdown(result["moderation_report"])

            st.subheader("ğŸ’¬ Suggested Reply")
            st.markdown(result["reply_suggestion"])

            st.subheader("â±ï¸ Latency")
            st.success(f"{latency} seconds")

        except Exception as e:
            st.error(f"âŒ Error: {str(e)}")
else:
    st.caption("ğŸ’¡ Enter a query and click the button to generate results.")

# ------------------ ğŸ§ª Manual Evaluation Samples ------------------
st.markdown("---")
st.subheader("ğŸ§ª Manual Evaluation Samples")

sample_label = st.selectbox("ğŸ§¾ Choose an evaluation case", list(sample_options.keys()))

if st.button("ğŸ§ª Run Sample Evaluation"):
    selected_case = sample_options[sample_label]
    thread_id = selected_case["thread_id"]
    user_query = selected_case["query"]
    expected_output = selected_case["expected"]

    with st.spinner("Evaluating agent outputs..."):
        start_time = time.time()
        response = run_graph(thread_id=thread_id, user_query=user_query)
        latency = round(time.time() - start_time, 2)

    st.markdown("### ğŸ“¥ Sample Input")
    st.write(f"**Thread ID:** `{thread_id}`\n\n**Query:** {user_query}")

    st.markdown("### âœ… Model Output")
    st.write(response.get("summary", "No summary available."))

    st.markdown("### ğŸ¯ Expected Behavior")
    st.info(expected_output)

    st.markdown("### â±ï¸ Latency")
    st.success(f"{latency} seconds")
