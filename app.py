import streamlit as st
from scripts.agent_graph import run_graph
from scripts.eval_samples import sample_options
import time

# ------------------ Streamlit Page Config ------------------
st.set_page_config(page_title="ThreadNavigatorAI", layout="wide")

# ------------------ Branding Header ------------------
st.markdown("""
# ğŸ§µ ThreadNavigatorAI
**Navigate Reddit-style threads using LLM agents**
""")
# st.image("assets/banner_threadnavigatorai.png", width=600)

# ------------------ Sidebar: Product Framing ------------------
with st.sidebar:
    st.markdown("## ğŸ’¡ Why ThreadNavigatorAI?")
    st.markdown("""
Reddit-style discussions are noisy:
- ğŸ˜’ Sarcasm, trolling, off-topic replies
- ğŸ¤¯ Difficult to summarize or moderate

### âœ… This app solves that:
- ğŸ§  Smart summarization using LLM agents
- ğŸ›¡ï¸ Moderation for bias, trolling, and tone
- ğŸ’¬ Personalized reply suggestions

### âš™ï¸ Built With:
- ğŸ§  LangGraph (Agent Orchestration)
- ğŸŒ OpenRouter (LLMs like Mistral)
- ğŸ§¾ Weaviate (Semantic Memory)
- ğŸ–¥ï¸ Streamlit (Free-tier UI)
    """)
    st.markdown("---")
    st.caption("Made by [Rajesh](https://github.com/rajesh1804) | Powered by OpenRouter + LangGraph")

# ------------------ Try Demo Button ------------------
if st.button("ğŸ”¥ Try Smart Demo"):
    thread_id = "thread_002"
    query = "Summarize this thread and point out any sarcasm or trolling"
    with st.spinner("Running LangGraph agents..."):
        try:
            start_time = time.time()
            result = run_graph(thread_id=thread_id, user_query=query)
            latency = round(time.time() - start_time, 2)

            st.success("âœ… Demo Complete!")
            st.subheader("ğŸ“ Smart Summary")
            st.markdown(result["summary"])

            st.subheader("ğŸ›¡ï¸ Moderation Report")
            st.markdown(result["moderation_report"])

            st.subheader("ğŸ’¬ Suggested Reply")
            st.markdown(result["reply_suggestion"])

            st.subheader("â±ï¸ Latency")
            st.success(f"{latency} seconds")

        except Exception as e:
            st.error(f"âŒ Error: {str(e)}")

st.markdown("---")

# ------------------ Dual Column Layout ------------------
col1, col2 = st.columns([1, 1])

# ------------------ ğŸ§  Agent Workflow Section ------------------
with col1:
    st.subheader("ğŸ§  Run Agentic Workflow")

    thread_id = st.selectbox("ğŸ“‚ Choose a thread", [
        "thread_001", "thread_002", "thread_003", "thread_004"])

    query = st.text_input("ğŸ’¬ Ask a question about the thread", 
                          placeholder="e.g. What's the main takeaway? Any bias?")

    if st.button("ğŸš€ Analyze Thread") and thread_id and query:
        with st.spinner("Running LangGraph pipeline..."):
            try:
                start_time = time.time()
                result = run_graph(thread_id=thread_id, user_query=query)
                latency = round(time.time() - start_time, 2)

                st.success("âœ… Analysis Complete!")
                st.markdown("### ğŸ“ Smart Summary")
                st.markdown(result["summary"])

                st.markdown("### ğŸ›¡ï¸ Moderation Report")
                st.markdown(result["moderation_report"])

                st.markdown("### ğŸ’¬ Suggested Reply")
                st.markdown(result["reply_suggestion"])

                st.markdown("### â±ï¸ Latency")
                st.success(f"{latency} seconds")

            except Exception as e:
                st.error(f"âŒ Error: {str(e)}")
    else:
        st.caption("ğŸ’¡ Enter a question and click Analyze Thread")

# ------------------ ğŸ§ª Manual Evaluation Section ------------------
with col2:
    st.subheader("ğŸ§ª Manual Evaluation Samples")

    sample_label = st.selectbox("ğŸ§¾ Choose an evaluation case", list(sample_options.keys()))

    if st.button("ğŸ§ª Run Sample Evaluation"):
        selected_case = sample_options[sample_label]
        thread_id = selected_case["thread_id"]
        user_query = selected_case["query"]
        expected_output = selected_case["expected"]

        with st.spinner("Evaluating with agents..."):
            try:
                start_time = time.time()
                response = run_graph(thread_id=thread_id, user_query=user_query)
                latency = round(time.time() - start_time, 2)

                st.markdown("### ğŸ“¥ Sample Input")
                st.code(f"Thread ID: {thread_id}\nQuery: {user_query}", language="text")

                st.markdown("### âœ… Model Output")
                st.markdown(response.get("summary", "No summary returned."))

                st.markdown("### ğŸ¯ Expected Behavior")
                st.info(expected_output)

                st.markdown("### â±ï¸ Latency")
                st.success(f"{latency} seconds")

            except Exception as e:
                st.error(f"âŒ Error: {str(e)}")